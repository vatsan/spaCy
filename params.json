{"name":"spaCy","tagline":"Lightning fast, full-cream NL tokenization for Python/Cython. Tokens are pointers to rich Lexeme structs.","body":"## Why did you write this?\r\n\r\nI've been doing NLP research for about ten years now, and I've hated every tokenizer I've ever used. They're inefficient, brittle, and inexpressive. spaCy takes a very different approach.\r\n\r\n## What does it do?\r\n\r\nGiven a string of text, get a list of token IDs, and use them to access a variety of properties. You'll almost never need to do additional string processing:\r\n\r\n```python\r\n    >>> import spacy\r\n    >>> from spacy import en\r\n    >>> string = u\"Features are calculated easily! And much more speedilyâ€¦\"\r\n    >>> tokens = en.tokenize(string)\r\n    >>> tokens\r\n    <spacy.tokens.Tokens object at 0x1041e6960>\r\n\r\n```\r\n\r\n## Why is it good?\r\n\r\n### Richer output\r\n\r\nInstead of strings, you get a pointer to a struct housing all sorts of pre-computed lexical-type information. Because our lexical tokens are pointers, we can give you all sorts of goodies:\r\n\r\n```python\r\n    >>> import spacy\r\n    >>> from spacy import en\r\n    >>> from spacy.lexeme import prob_of, cluster_of, can_noun, can_verb # ...More\r\n    >>> Apples, grappled, oranges = en.tokenize(u\"Apples grappled oranges\")\r\n    >>> # Load your own unigram probabilities, or use my Good-Turing smoothed estimates from Wikipedia.\r\n    >>> spacy.prob_of(Apples)\r\n    -30.6548\r\n    >>> # We store a bit-field of possible tags. Again, load your own data, or use good defaults.\r\n    >>> can_noun(Apples), can_verb(Apples), can_verb(grappled)\r\n    True False True\r\n    >>> # Integer of Brown cluster bit-string. Similar words receive better-than-chance similar values.\r\n    >>> (cluster_of(Apples) - cluster_of(grappled)) > cluster_of(oranges) - cluster_of(grappled)\r\n```\r\n\r\n### Faster\r\n\r\nAnd spaCy returns you pointers to these rich lexical types much faster than a normal tokenizer can give you a list of strings: \r\n\r\n| System \t| Time  \t| Words/second \t| Speed Factor\r\n|--------\t|-------\t|--------------\t|--------\r\n| NLTK   \t| 5m37s \t| 89,000       \t| 1.00\r\n| spaCy  \t| 16s   \t| 1,875,000    \t| 16.00\r\n(A quick experiment on 30m words, or about 100mb gzipped text, of the Gigaword corpus. I'll do something more formal with other string-based implementations soon. The tokenizer.sed script is much faster than NLTK, since it goes in a single pass, but still about 4x slower than spaCy.)\r\n\r\nspaCy is also 100% reversible/non-destructive: you can always recover indices into the original string, which is always difficult and sometimes impossible with standard tokenizers. Want to display text with inline mark-up? We have you covered. Other tokenizers? Not so much.\r\n\r\n## What is this sorcery? (Or, how does it work)\r\n\r\nThere are two things you need to know to understand how and why spaCy works:\r\n\r\n1. Zipf's law. The frequency distribution of word types is heavily skewed. The number of word tokens in a sample of text grows exponentially faster than the number of types. i.e., vocabularies are much smaller than texts.\r\n2. Cython. A language that lets us write code that seamlessly mixes low-level pointer semantics, and high level Python constructs. \r\n\r\nWhat we do is iterate through the characters in the text, and look up white-space delimited chunks in a global hash table. The table returns pointers to nodes in a linked-list, which we traverse to extend our vector of tokens. Like this:\r\n\r\n```cython\r\n    cpdef Tokens tokenize(self, unicode string):\r\n        cdef size_t length = len(string)\r\n        cdef Py_UNICODE* characters = <Py_UNICODE*>string\r\n\r\n        cdef size_t i\r\n        cdef Py_UNICODE c\r\n\r\n        cdef Tokens tokens = Tokens(self)\r\n        cdef Py_UNICODE* current = <Py_UNICODE*>calloc(len(string), sizeof(Py_UNICODE))\r\n        cdef size_t word_len = 0\r\n        cdef Lexeme* token\r\n        for i in range(length):\r\n            c = characters[i]\r\n            # Look for whitespace-delimited chunks, e.g.\r\n            # Hi! world\r\n            # is two chunks with three tokens.\r\n            if _is_whitespace(c):\r\n                if word_len != 0:\r\n                    # Lookup the chunk in a hash table. Even with punctuation\r\n                    # and affixes, almost all chunks are previously seen.\r\n                    token = <Lexeme*>self.lookup(-1, current, word_len)\r\n                    # Get back a pointer to a linked list node, which we traverse to\r\n                    # get all the tokens from the chunk.\r\n                    while token != NULL:\r\n                        # A Token is a pointer to a Lexeme struct.\r\n                        tokens.append(<Lexeme_addr>token)\r\n                        token = token.tail\r\n                        for j in range(word_len+1):\r\n                            current[j] = 0\r\n                    word_len = 0\r\n            else:\r\n                current[word_len] = c\r\n                word_len += 1\r\n        # Do the last chunk.\r\n        if word_len != 0:\r\n            token = <Lexeme*>self.lookup(-1, current, word_len)\r\n            while token != NULL:\r\n                tokens.append(<Lexeme_addr>token)\r\n                token = token.tail\r\n        free(current)\r\n        return tokens\r\n\r\n```\r\n\r\nBecause of Zipf's law, almost every call to lookup is served by the hash-table. It's only when we see a previously unseen chunk that we must look within the string and create new Lexeme structs. By avoiding calls to that process, we're free to do more sophisticated processing, and use a simpler, more maintainable and reliable implementation than we would otherwise.\r\n\r\nA chunk is any sequence of non-whitespace characters, e.g. \"(Hello!)\" is a single chunk. When we encounter a previously-unseen chunk, we create a Lexeme for the first substring, \"(\", which knows its verbatim string (\"(Hello!)\". We then lookup the other substrings,  \"Hello!)\", \"!)\", and \")\", creating new Lexeme structs when we encounter one we haven't seen.\r\n\r\nBy maintaining a global vocabulary, each token is represented by a 64-bit integer, which we can easily use in numpy arrays, vectors, etc. But those 64-bit integers instantly decode to a rich representation, giving us everything we need to calculate features. And remember: our vocabulary of seen chunks is exponentially smaller than the number of tokens we have processed.\r\n\r\nDespite the relatively friendly syntax, all of our operations are low-level. The call to _is_whitespace is implemented as a Cython conditional, which compiles into an inlined C++ case-switch statement. This case-switch statement is the only thing that's executed on every character. Once we see a whole chunk, we hash it (using MurmurHash), and look it up in a hash table (Google's dense_hash_map), which serves us a pointer. On the rare occasions that we encounter an unseen chunk, we can apply Python string-processing functions at our leisure.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}